{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Damped harmonic oscillator using PINNs\n",
    "\n",
    "\n",
    "In this notebook, we'll replicate the code and animations in [this video](https://youtu.be/IDIv92Z6Qvc?si=EKxJZZeUdce1jgF1) by remixing [this notebook](https://github.com/benmoseley/DLSC-2023/blob/main/lecture-5/PINN%20demo.ipynb) to solve simulation and inversion problems related to the damped harmonic oscillator. Actually I'm not very good at making animations, so we'll just use the animation code in [this notebook](https://github.com/benmoseley/harmonic-oscillator-pinn/blob/main/Harmonic%20oscillator%20PINN.ipynb) :)\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image as PILImage\n",
    "from IPython.display import display, Image, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable, List, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "# elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "#     device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory to store animation frames\n",
    "PLOT_DIR = \"plots\"\n",
    "os.makedirs(PLOT_DIR, exist_ok=True)\n",
    "\n",
    "def save_fig(plt, filename: str) -> None:\n",
    "    \"Helper function for saving figures\"\n",
    "    path = os.path.join(PLOT_DIR, filename)\n",
    "    plt.savefig(path, bbox_inches=\"tight\", pad_inches=0.1, dpi=100, facecolor=\"white\")\n",
    "\n",
    "def save_gif_PIL(outfile: str, files: List[str], fps: int=5, loop: int=0) -> None:\n",
    "    \"Helper function for saving GIFs\"\n",
    "    paths = [os.path.join(PLOT_DIR, file) for file in files]\n",
    "    imgs = [PILImage.open(path) for path in paths]\n",
    "    print(f\"Saving {len(imgs)} frames to {outfile}\")\n",
    "    imgs[0].save(fp=outfile, format=\"GIF\", append_images=imgs[1:], save_all=True, duration=int(1000/fps), loop=loop)\n",
    "    for path in paths: os.remove(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a spring with a spring constant $k$ and a mass $m$ attached to it. The system is subject to damping, characterized by a damping coefficient $\\mu$. The distance from equilibrium as a function of time is $u(t)$. \n",
    "\n",
    "The inertial force of the mass is given by Newton's Second Law:\n",
    "\\begin{equation*}\n",
    "F_{\\text{inertia}} = m \\frac{d^2u}{dt^2}\n",
    "\\end{equation*}\n",
    "\n",
    "The restoring force exerted by the spring is given by Hooke's Law:\n",
    "\n",
    "\\begin{equation*}\n",
    "F_{\\text{spring}} = -ku\n",
    "\\end{equation*}\n",
    "\n",
    "The damping force opposing the velocity of the mass is given by:\n",
    "\\begin{equation*}\n",
    "F_{\\text{damping}} = -\\mu \\frac{du}{dt}\n",
    "\\end{equation*}\n",
    "\n",
    "By Newton's Second Law, we know that the sum of all forces on the mass should be equal to the mass times the acceleration:\n",
    "\\begin{align*}\n",
    "F_{\\text{inertia}} &= F_{\\text{spring}} + F_{\\text{damping}} \\\\\n",
    "m \\frac{d^2u}{dt^2} &= -ku -\\mu \\frac{du}{dt} \\\\\n",
    "m \\frac{d^2u}{dt^2} &+ \\mu \\frac{du}{dt} + ku = 0 \\\\\n",
    "\\end{align*}\n",
    "\n",
    "We can factor out $u$\n",
    "\n",
    "$$\n",
    "\\left(m \\frac{d^2}{dt^2} + \\mu \\frac{d}{dt} + k \\right) \\big[u \\big] = 0 \\\\\n",
    "$$\n",
    "\n",
    "to put the equation in the form\n",
    "\n",
    "$$\n",
    "\\mathcal{D}[u(x, t)] = f(x, t)\n",
    "$$\n",
    "\n",
    "where $\\mathcal{D}$ is some differential operator, and $f(x, t)$ represents the external forcing function or input.\n",
    "\n",
    "We will focus on solving the problem in the under-damped state, i.e. where the oscillation is slowly damped by friction \n",
    "\n",
    "Mathematically, this occurs when:\n",
    "\\begin{equation*}\n",
    "\\delta < \\omega_0~,~~~~~\\mathrm{where}~~\\delta = \\dfrac{\\mu}{2m}~,~\\omega_0 = \\sqrt{\\dfrac{k}{m}}~.\n",
    "\\end{equation*}\n",
    "\n",
    "Furthermore, we consider the following initial conditions of the system:\n",
    "\\begin{equation*}\n",
    "u(t=0) = 1, \\quad \\dfrac{d u}{d t}(t=0) = 0.\n",
    "\\end{equation*}\n",
    "\n",
    "meaning that the mass starts extended at rest. For this particular case, the exact solution is known and given by:\n",
    "\\begin{equation*}\n",
    "u(t) = e^{-\\delta t}(2 A \\cos(\\phi + \\omega t))~,~~~~~\\mathrm{with}~~\\omega=\\sqrt{\\omega_0^2 - \\delta^2}~.\n",
    "\\end{equation*}\n",
    "\n",
    "For a more detailed mathematical description of the harmonic oscillator, check out [this blog post](https://beltoforion.de/en/harmonic_oscillator/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_solution(d: float, w0: float, t: float) -> float:\n",
    "    \"Defines the analytical solution to the under-damped harmonic oscillator problem above.\"\n",
    "    assert d < w0\n",
    "    w = np.sqrt(w0**2 - d**2)\n",
    "    phi = np.arctan(-d/w)\n",
    "    A = 1/(2*np.cos(phi))\n",
    "    cos = torch.cos(phi +w*t)\n",
    "    exp = torch.exp(-d*t)\n",
    "    u = exp*2*A*cos\n",
    "    return u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow overview\n",
    "\n",
    "There are **two scientific tasks** related to the harmonic oscillator we will use a PINN for:\n",
    "\n",
    ">First, we will **simulate** the system using a PINN, given its initial conditions.\n",
    "\n",
    ">Second, we will **invert** for underlying parameters of the system using a PINN, given some noisy observations of the oscillator's displacement.\n",
    "\n",
    ">Third, we will compare the performance of a PINN and an MLP at learning an oscillatory pattern from partial data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 1: train a PINN to simulate the system\n",
    "\n",
    "---\n",
    "\n",
    "### Theory\n",
    "\n",
    "Using traditional PDE solving techniques (finite difference, finite element, etc.), we have to discretize the input and output space. This means that the output will be a vector (or maybe matrix) of values corresponding to the PDE solution at the given input points. Then to craft a higher-precision solution, we will necessarily need to feed in more input points, requiring more computation.\n",
    "\n",
    "PINNs are cool because the trained neural network is an approximatino of the PDE solution itself. This means that you actually get a continuous function out that can be queried over any point within the domain. Note, however, that PINNs may take a long time to train depending on the dimensionality of the input space, and may not be required for simple PDEs... like the one above :)\n",
    "\n",
    "A PINN is trained to directly approximate the solution to the differential equation, i.e.\n",
    "\n",
    "$$\n",
    "N\\!N(x, t;\\theta) \\approx u(x, t),\n",
    "$$\n",
    "\n",
    "given a PDE and its boundary/initial conditions\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{D}[u(x, t)] &= f(x, t), \\quad x \\in \\Omega \\subset \\mathbb{R}^d \\\\\n",
    "B_k[u(x, t)] &= g_k(x, t), \\quad x \\in \\Gamma_k \\subset \\partial \\Omega\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $\\partial \\Omega$ is the boundary of domain $\\Omega$, and $\\Gamma_k$ are specific boundary regions within $\\partial \\Omega$\n",
    "- $\\mathcal{D}$ is a differential operator that governs the behavior of the system\n",
    "- $B_k$ are a set of boundary operators that impose specific conditions at the boundaries of the domain $\\Omega$, $\\partial \\Omega$\n",
    "- $f(x, t)$ is a source term or function that represents any external influence affecting the solution within the domain $\\Omega$\n",
    "- $g_k(x, t)$ specifies the conditions imposed by the boundary operators at the boundaries $\\Gamma_k$\n",
    "- $u(x, t)$ is the solution to the PDE\n",
    "\n",
    "We will actually rewrite these as\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{D}[u(x, t)] - f(x, t) &= 0 \\\\\n",
    "B_k[u(x, t)] - g_k(x, t) &= 0\n",
    "\\end{align*}\n",
    "\n",
    "to incorporate them into the loss function. This way we can start to see where the notion of loss actually comes into play: *we want our learned solution $u(x, t)$ to satisfy the boundary/initial conditions when acted upon by the corresponding operators*.\n",
    "\n",
    "PINNs train a neural network to approximate the solution to the PDE $N\\!N(x, t;\\theta) \\approx u(x, t)$ using the following loss function:\n",
    "\n",
    "$$\n",
    "L(\\theta) = L_b(\\theta) + L_p(\\theta)\n",
    "$$\n",
    "\n",
    "The supervised **boundary loss** $ L_b(\\theta)$ is given by\n",
    "\n",
    "\n",
    "$$\n",
    "L_b(\\theta) = \\sum_{k} \\frac{\\lambda_k}{N_{bk}} \\sum_{j}^{N_{bk}} \\left\\| B_k \\left[ NN(x_{kj}, t_j; \\theta) \\right] - g_k(x_{kj}, t_j) \\right\\|^2 \n",
    "$$\n",
    "\n",
    "where the outer summation with index $k$ is over the various given boundary conditions $B_k$ on the PDE. $N_{bk}$ is the number of *boundary* training points $x_{kj}$ for boundary condition $B_k$. Then the inner summation over $j$ sums the $N_{bk}$ squared residuals, and $\\lambda_k$ is the regularization coefficient for the corresponding boundary condition.\n",
    "\n",
    "The unsupervised **physics loss** $ L_p(\\theta)$ is given by \n",
    "$$\n",
    "L_p(\\theta) = \\frac{1}{N_p} \\sum_{i}^{N_p} \\left\\| \\mathcal{D} \\left[ NN(x_i, t_i; \\theta) \\right] - f(x_i, t_i) \\right\\|^2 \n",
    "$$\n",
    "\n",
    "where we sum the $N_p$ squared residuals. If the neural network is perfectly physical, then this loss should go to zero. However, in reality this loss function will only promote that the network is physical, and will remain nonzero since it competes with the other loss functions.\n",
    "\n",
    "The physics loss $L_p(\\theta)$ is trained on so-called collocation points, and is considered to be unsupervised because no points are labeled. The collocation points are either randomly or uniformly chosen to train the network within the domain $\\Omega$. This loss term acts as a regularizer to restrict the neural net function approximation $N\\!N(x, t;\\theta) \\approx u(x, t)$ to a physically-accurate solution based on the PDE. Regularizing terms, in general, are used to assert preferences toward sets of desired functions.  In our case, we know that the spring equation should be oscillatory, and can move the model toward learning this solution with the damped harmonic oscillator PDE.\n",
    "\n",
    "The boundary loss $L_b(\\theta)$ is considered to be supervised because it consists of a set of labeled training points, our boundary conditions, that we want our learned function to satisfy. In this way, it sets itself apart from the physics loss term. $L_p(\\theta)$ helps ensure our solution approximates the general PDE, and $L_b(\\theta)$ helps ensure that our solution is unique. Think of it this way: when we integrate a function, we get a constant of integration and a general solution. By providing boundary conditions, we can choose the particular solution for the problem we're trying to solve.\n",
    "\n",
    "\n",
    "If the unsupervised physics loss is too big, and the supervised boundary loss is around zero, then the network will learn a random particular solution of the PDE equation -- it won't match the given boundary conditions\n",
    "\n",
    "If the supervised boundary loss is too big, and the unsupervised physics loss is around zero, then the collocation points won't matter and the network won't learn the oscillatory process.\n",
    "\n",
    "The lambda regularization coefficients are used to help address this issue.\n",
    "\n",
    "One more thing: $\\tanh$ is preferred over $\\text{ReLU}$ in PINNs because it's smooth and infinitely differentiable without loss of signal (during nested application) near zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Task\n",
    "\n",
    "The first task is to use a PINN to **simulate** the system.\n",
    "\n",
    "Specifically, our inputs and outputs are:\n",
    "\n",
    "- Inputs: underlying differential equation and the initial conditions of the system\n",
    "- Outputs: estimate of the solution, $u(t)$\n",
    "\n",
    "### Approach\n",
    "\n",
    "The PINN is trained to directly approximate the solution to the differential equation, i.e.\n",
    "\n",
    "$$\n",
    "N\\!N(t;\\theta) \\approx u(t)~,\n",
    "$$\n",
    "\n",
    "For this task, we use $\\delta=2$, $\\omega_0=20$, $m=1$, and try to learn the solution over the domain $t\\in [0,1]$.\n",
    "\n",
    "### Setting up the loss function\n",
    "\n",
    "We have equation\n",
    "\n",
    "$$\n",
    "\\left(m \\frac{d^2}{dt^2} + \\mu \\frac{d}{dt} + k \\right) \\big[u(t) \\big] = 0\n",
    "$$\n",
    "\n",
    "with boundary/initial conditions\n",
    "\n",
    "\\begin{align*}\n",
    "1)& \\quad u(t=0) = 1 \\quad \\Rightarrow \\quad u(t=0) - 1 = 0 \\\\\n",
    "2)& \\quad \\dfrac{d u}{d t}(t=0) = 0 \\quad \\Rightarrow \\quad \\dfrac{d u}{d t}(t=0) - 0 = 0\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Then for the damped harmonic oscillator PDE and its initial conditions, To simulate the system, the PINN is trained with the following loss function:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(\\theta) &= L_b(\\theta) + L_p(\\theta) \\\\\n",
    "&= (N\\!N(0;\\theta) - 1)^2 + \\lambda_1 \\left(\\frac{d N\\!N}{dt}(0;\\theta) - 0\\right)^2 + \\frac{\\lambda_2}{N} \\sum^{N}_{i} \\left( \\left[ m\\frac{d^2}{dt^2} + \\mu \\frac{d}{dt} + k \\right] N\\!N(t_{i};\\theta)  \\right)^2\n",
    "\\end{align*}\n",
    "\n",
    "### Computing gradients\n",
    "\n",
    "Autograd can compute derivatives on the weights, and since it is an entire computational graph, \n",
    "we can also calculate the gradients of the neural network output with respect to its inputs, $x$ and/or $t$, using `torch.autograd.grad`. For this reason, we can solve for terms of the PDE like $\\frac{\\partial u}{\\partial t}$, where we take the derivative of the network output, $u$, with respect to its inputs, $t$.\n",
    "\n",
    "\n",
    "When we use it for something like $u_t$, we want the derivative of $u$ with respect to boundary point $t$.\n",
    "Under the hood, autograd is computing the vector-jacobian product in reverse-mode, not a series of jacobian matrix products like in forward-mode. If $u$ is not scalar, we also have to define the vector that we're applying to that jacobian in the vector-jacobian product. In this case, since it is scalar, but because the shape is weird, we'll do a torch.ones_like(u). This is actually a very [good video](https://youtu.be/q8bAXSBRz3k?si=drrnUXs-y5oFKkGQ&t=4139) on backpropagation.\n",
    "\n",
    "In PINNs, most time is spent computing higher-order derivatives, not the forward pass.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eventually abstract the PINN MLP layer using this or something similar\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_input, num_hidden, num_output, num_layers, activation=nn.Tanh):\n",
    "        super(MLP, self).__init__() \n",
    "        self.activation = activation()\n",
    "        self.input_layer = nn.Linear(num_input, num_hidden)\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Linear(num_hidden, num_hidden) for _ in range(num_layers-1)\n",
    "        ])\n",
    "        self.output_layer = nn.Linear(num_hidden, num_output)\n",
    "\n",
    "    def forward(self, t):\n",
    "        t = self.activation(self.input_layer(t))\n",
    "        for layer in self.hidden_layers:\n",
    "            t = self.activation(layer(t))\n",
    "        u = self.output_layer(t)\n",
    "        return u\n",
    "\n",
    "class Collocation:\n",
    "    def __init__(\n",
    "            self, \n",
    "            domain: torch.Tensor,\n",
    "            value: Union[Callable, float],\n",
    "            coefficients: List[float] = [1.0],\n",
    "            highest_order: int = 0,\n",
    "            reg_coeff: float = 1.0,\n",
    "            ):\n",
    "        # minus one to account for zeroeth order term\n",
    "        assert len(coefficients)-1 == highest_order\n",
    "        self.value = value\n",
    "        self.domain = domain\n",
    "        self.highest_order = highest_order  \n",
    "        self.coefficients = coefficients\n",
    "        self.reg_coeff = reg_coeff\n",
    "\n",
    "    def loss(self, model):\n",
    "        # don't like passing the model inside here... oh well\n",
    "        u_physics = model(self.domain)\n",
    "\n",
    "        # the order of the time derivative corresponds to its index in the list\n",
    "        u_derivs = [u_physics]  # zeroeth order derivative is just the function itself\n",
    "\n",
    "        # calculate n-th order derivatives up to the highest_order\n",
    "        if self.highest_order > 0:\n",
    "            for n in range(1, self.highest_order+1):\n",
    "                prev_deriv = u_derivs[n-1] # can also index with -1 since it will be the end of the list\n",
    "                nth_deriv = torch.autograd.grad(prev_deriv, self.domain, torch.ones_like(prev_deriv), create_graph=True)[0]\n",
    "                u_derivs.append(nth_deriv)\n",
    "        \n",
    "        # the coefficients and derivs sorted in ascending order by degree\n",
    "        pde_lhs = sum(self.coefficients[i]*u_derivs[i] for i in range(self.highest_order))\n",
    "        loss = torch.mean((pde_lhs - self.value)**2)\n",
    "        return self.reg_coeff * loss\n",
    "    \n",
    "\n",
    "class BoundaryCondition:\n",
    "    def __init__(\n",
    "            self, \n",
    "            domain: torch.Tensor,\n",
    "            value: Union[Callable, float] = None,\n",
    "            deriv_value: Union[Callable, float] = None,  # this should be turned into a list of ascending order derivatives functions\n",
    "            reg_coeff: float = 1.0\n",
    "            ):\n",
    "        self.domain = domain # eventually make this an interval and use something like arange or linspace to get them usig\n",
    "        self.value = value # eventually do the same here -- make this and reg_coeff torch tensors so they can quickly multiplied\n",
    "        self.deriv_value = deriv_value\n",
    "        self.reg_coeff = reg_coeff\n",
    "\n",
    "    def loss(self, model):\n",
    "        # don't like passing the model inside here... oh well\n",
    "        u_boundary = model(self.domain) # (1, 1)\n",
    "\n",
    "        # the loss is the residual between the model prediction and the set value\n",
    "        if self.value is not None:\n",
    "            loss = (torch.squeeze(u_boundary) - self.value)**2\n",
    "        if self.deriv_value is not None:\n",
    "            ut = torch.autograd.grad(u_boundary, self.domain, torch.ones_like(u_boundary), create_graph=True)[0] # (1, 1)\n",
    "            loss = (torch.squeeze(ut) - self.deriv_value)**2\n",
    "\n",
    "        return self.reg_coeff * loss\n",
    "    \n",
    "\n",
    "class PINN(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            num_input: int, \n",
    "            num_hidden: int, \n",
    "            num_output: int, \n",
    "            num_layers: int, \n",
    "            collocation: Collocation,\n",
    "            boundary_conditions: List[BoundaryCondition],\n",
    "            activation: Callable = nn.Tanh\n",
    "            ):\n",
    "        super(PINN, self).__init__() \n",
    "        self.activation = activation()\n",
    "        self.collocation = collocation\n",
    "        self.boundary_conditions = boundary_conditions\n",
    "        self.input_layer = nn.Linear(num_input, num_hidden)\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Linear(num_hidden, num_hidden) for _ in range(num_layers-1)\n",
    "        ])\n",
    "        self.output_layer = nn.Linear(num_hidden, num_output)\n",
    "        \n",
    "    def forward(self, t):\n",
    "        \"\"\"Forward pass representing the PDE solution\"\"\"\n",
    "        t = self.activation(self.input_layer(t))\n",
    "        for layer in self.hidden_layers:\n",
    "            t = self.activation(layer(t))\n",
    "        u = self.output_layer(t)\n",
    "        return u\n",
    "\n",
    "    def loss(self):\n",
    "        \"\"\"Return the loss function for the PDE\"\"\"\n",
    "\n",
    "        # comput the boundary loss\n",
    "        boundary_loss =  0.0\n",
    "        for bc in self.boundary_conditions:\n",
    "            boundary_loss += bc.loss(self)\n",
    "\n",
    "        # compute the physics loss\n",
    "        physics_loss = self.collocation.loss(self)\n",
    "\n",
    "        return boundary_loss + physics_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants \n",
    "m = 1       # mass, kg\n",
    "w0 = 20     # natural frequency, rad/s (sqrt(k/m))\n",
    "k = w0**2   # spring constant, N/m\n",
    "d = 2       # damping ratio (mu/(2m))\n",
    "mu = 2*d*m  # damping coefficient, Ns/m\n",
    "\n",
    "# store the coefficients for the highest order derivatives\n",
    "coeffs = [k, mu, m]  # coefficientss on [u, ut, utt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define boundary points, for the boundary loss\n",
    "t_boundary = torch.tensor(0.).view(-1,1).requires_grad_(True).to(device)# (1, 1)\n",
    "\n",
    "# define training points over the entire domain, for the physics loss\n",
    "t_physics = torch.linspace(0,1,30).view(-1,1).requires_grad_(True).to(device)# (30, 1)\n",
    "\n",
    "# define the testing points for the plot, and get the exact solution\n",
    "t_test = torch.linspace(0,1,300).view(-1,1).to(device)\n",
    "u_exact = exact_solution(d, w0, t_test).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the collocation points\n",
    "coll = Collocation(t_physics, value=0.0, coefficients=coeffs, highest_order=2, reg_coeff=1e-4)\n",
    "\n",
    "# set the boundary conditions\n",
    "bc1 = BoundaryCondition(t_boundary, value=1.0, reg_coeff=1.0)\n",
    "bc2 = BoundaryCondition(t_boundary, deriv_value=0.0, reg_coeff=1e-4)\n",
    "bcs = [bc1, bc2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn = PINN(\n",
    "    num_input=1,\n",
    "    num_hidden=32,\n",
    "    num_output=1, \n",
    "    num_layers=3,\n",
    "    collocation=coll,\n",
    "    boundary_conditions=bcs\n",
    "    ).to(device)\n",
    "optimizer = torch.optim.Adam(pinn.parameters(),lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(u, t_physics, t_boundary, t_test, u_exact, i):\n",
    "    plt.figure(figsize=(6,2.5))\n",
    "\n",
    "    # set limits -- the x limit is basically set by the collocation points\n",
    "    # but the y-axis grows/shrinks a bit as the solution changes -- so fix it\n",
    "    plt.ylim(-0.8, 1.1)\n",
    "\n",
    "    # plot the boundary points in red and collocation points in green\n",
    "    plt.scatter(t_physics[:,0], \n",
    "                torch.zeros_like(t_physics)[:,0], s=20, lw=0, color=\"tab:green\", alpha=0.6, label=\"Collocation points\")\n",
    "    plt.scatter(t_boundary[:,0], \n",
    "                torch.zeros_like(t_boundary)[:,0], s=20, lw=0, color=\"tab:red\", alpha=0.6, label=\"Boundary points\")\n",
    "    \n",
    "    # plot the PINN solution and exact solution\n",
    "    plt.plot(t_test[:,0], u_exact[:,0], label=\"Exact solution\", color=\"tab:grey\", alpha=0.6)\n",
    "    plt.plot(t_test[:,0], u[:,0], label=\"PINN solution\", color=\"tab:blue\")\n",
    "    \n",
    "\n",
    "    plt.title(f\"Training step {i}\")\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"u(t)\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# save the filenames of each plot frame\n",
    "files = []\n",
    "\n",
    "num_iters = 15001\n",
    "for i in range(num_iters):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # compute each term of the PINN loss function above\n",
    "    # using the following hyperparameters\n",
    "    lambda1, lambda2 = 1e-1, 1e-4\n",
    "    \n",
    "    # compute boundary loss\n",
    "    u = pinn(t_boundary)# (1, 1)\n",
    "    loss1 = (torch.squeeze(u) - 1)**2\n",
    "    ut = torch.autograd.grad(u, t_boundary, torch.ones_like(u).to(device), create_graph=True)[0]# (1, 1)\n",
    "    loss2 = (torch.squeeze(ut) - 0)**2\n",
    "    \n",
    "    # compute physics loss\n",
    "    u = pinn(t_physics)# (30, 1)\n",
    "    ut = torch.autograd.grad(u, t_physics, torch.ones_like(u).to(device), create_graph=True)[0]# (30, 1)\n",
    "    utt = torch.autograd.grad(ut, t_physics, torch.ones_like(ut), create_graph=True)[0]# (30, 1)\n",
    "    loss3 = torch.mean((utt + mu*ut + k*u)**2)\n",
    "    \n",
    "    # backpropagate joint loss, take optimizer step\n",
    "    loss = loss1 + lambda1*loss2 + lambda2*loss3\n",
    "    # loss = pinn.loss()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"loss: {loss.item():.4f}  | step: {i} / {num_iters}\", end=\"\\r\")\n",
    "\n",
    "    # plot the result as training progresses\n",
    "    if i % 100 == 0: \n",
    "        u = pinn(t_test)\n",
    "        plot_result(\n",
    "            u.detach().cpu(), \n",
    "            t_physics.detach().cpu(), \n",
    "            t_boundary.detach().cpu(),\n",
    "            t_test.detach().cpu(),\n",
    "            u_exact.detach().cpu(),\n",
    "            i\n",
    "        )\n",
    "\n",
    "        file = f\"pinn1_{i+1}.png\"\n",
    "        save_fig(plt, file)\n",
    "        files.append(file)\n",
    "        \n",
    "        if (i+1) % 6000 == 0: plt.show()\n",
    "        else: plt.close(\"all\")\n",
    "            \n",
    "save_gif_PIL(\"pinn1.gif\", files, fps=20, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"pinn1.gif\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Image(filename=\"pinn1.gif\")\n",
    "display(HTML('<img src=\"pinn1.gif\" />')) # to display on github"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 2: train a PINN to invert for underlying parameters\n",
    "\n",
    "---\n",
    "\n",
    "### Task\n",
    "\n",
    "The second task is to use a PINN to **invert** for underlying parameters.\n",
    "\n",
    "Specifically, our inputs and outputs are:\n",
    "\n",
    "- Inputs: noisy observations of the oscillator's displacement, $u_{\\mathrm{obs}}$\n",
    "- Outputs: estimate $\\mu$, the coefficient of friction\n",
    "\n",
    "### Approach\n",
    "\n",
    "Similar to above, the PINN is trained to directly approximate the solution to the differential equation, i.e.\n",
    "\n",
    "$$\n",
    "N\\!N(t;\\theta) \\approx u(t)~,\n",
    "$$\n",
    "\n",
    "However here we assume $\\mu$ is **not known** and we treat it as an additional **learnable parameter** when training the PINN.\n",
    "\n",
    "### Loss function\n",
    "\n",
    "The PINN is trained with the loss function:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta, \\mu)= \\frac{1}{N} \\sum^{N}_{i} \\left( \\left[ m\\frac{d^2}{dt^2} + \\mu \\frac{d}{dt} + k \\right] N\\!N(t_{i};\\theta)  \\right)^2 + \\frac{\\lambda}{M} \\sum^{M}_{j} \\left( N\\!N(t_{j};\\theta) - u_{\\mathrm{obs}}(t_{j}) \\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants \n",
    "m = 1       # mass, kg\n",
    "w0 = 20     # natural frequency, rad/s (sqrt(k/m))\n",
    "k = w0**2   # spring constant, N/m\n",
    "d = 2       # damping ratio (mu/(2m))\n",
    "# mu = ?    # parameter to be learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a PINN\n",
    "pinn = PINN(\n",
    "    num_input=1,\n",
    "    num_hidden=32,\n",
    "    num_output=1, \n",
    "    num_layers=3,\n",
    "    collocation=coll,\n",
    "    boundary_conditions=bcs\n",
    "    ).to(device)\n",
    "\n",
    "# treat mu as a learnable parameter, add it to optimizer\n",
    "mu = torch.nn.Parameter(torch.zeros(1, requires_grad=True))\n",
    "optimizer = torch.optim.Adam(list(pinn.parameters())+[mu], lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, create some noisy observational data\n",
    "torch.manual_seed(123)\n",
    "print(f\"True value of mu: {2*d}\")\n",
    "t_obs = torch.rand(40).view(-1,1)\n",
    "u_obs = exact_solution(d, w0, t_obs) + 0.04*torch.randn_like(t_obs)\n",
    "t_test = torch.linspace(0,1,300).view(-1,1)\n",
    "u_exact = exact_solution(d, w0, t_test)\n",
    "\n",
    "plt.figure(figsize=(6,2.5))\n",
    "plt.title(\"Noisy observational data\")\n",
    "plt.scatter(t_obs[:,0], u_obs[:,0])\n",
    "plt.plot(t_test[:,0], u_exact[:,0], label=\"Exact solution\", color=\"tab:grey\", alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(u, t_obs, u_obs, t_test, mus, i):\n",
    "    plt.figure(figsize=(12,2.5))\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.scatter(t_obs[:,0], u_obs[:,0], label=\"Noisy observations\", alpha=0.6, color=\"tab:blue\")\n",
    "    plt.plot(t_test[:,0], u[:,0], label=\"PINN solution\", color=\"tab:green\")\n",
    "    plt.title(f\"Training step {i}\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title(\"$\\mu$\")\n",
    "    plt.plot(mus, label=\"PINN estimate\", color=\"tab:green\")\n",
    "    plt.hlines(2*d, 0, len(mus), label=\"True value\", color=\"tab:grey\")\n",
    "    plt.xlabel(\"Training step\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# save the filenames of each plot frame\n",
    "files = [] \n",
    "\n",
    "# save the value of mu at each step\n",
    "mus = []\n",
    "\n",
    "num_iters = 15001\n",
    "for i in range(num_iters):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # compute each term of the PINN loss function above\n",
    "    # using the following hyperparameters\n",
    "    lambda1 = 1e4\n",
    "    \n",
    "    # compute physics loss\n",
    "    u = pinn(t_physics)# (30, 1)\n",
    "    ut = torch.autograd.grad(u, t_physics, torch.ones_like(u), create_graph=True)[0]# (30, 1)\n",
    "    utt = torch.autograd.grad(ut, t_physics, torch.ones_like(ut), create_graph=True)[0]# (30, 1)\n",
    "    loss1 = torch.mean((utt + mu*ut + k*u)**2)\n",
    "    \n",
    "    # compute data loss\n",
    "    u = pinn(t_obs)\n",
    "    loss2 = torch.mean((u - u_obs)**2)\n",
    "    \n",
    "    # backpropagate joint loss, take optimizer step\n",
    "    loss = loss1 + lambda1*loss2\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # record mu value\n",
    "    mus.append(mu.item())\n",
    "\n",
    "    print(f\"loss: {loss.item():.4f}  | step: {i} / {num_iters}\", end=\"\\r\")\n",
    "    \n",
    "    # plot the result as training progresses\n",
    "    if i % 150 == 0: \n",
    "        u = pinn(t_test)\n",
    "        plot_result(\n",
    "            u.detach().cpu(), \n",
    "            t_obs.detach().cpu(), \n",
    "            u_obs.detach().cpu(),\n",
    "            t_test.detach().cpu(),\n",
    "            mus,\n",
    "            i\n",
    "        )\n",
    "\n",
    "        file = f\"pinn2_{i+1}.png\"\n",
    "        save_fig(plt, file)\n",
    "        files.append(file)\n",
    "        \n",
    "        if (i+1) % 6000 == 0: plt.show()\n",
    "        else: plt.close(\"all\")\n",
    "            \n",
    "save_gif_PIL(\"pinn2.gif\", files, fps=20, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"pinn2.gif\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML('<img src=\"pinn2.gif\" />'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 3: PINN vs MLP \n",
    "\n",
    "---\n",
    "\n",
    "### Task\n",
    "\n",
    "The third task is to compare the performance of a PINN and an MLP at learning an oscillatory pattern from partial data. We'll use the same setup as in task 1, except this time we'll provide a few labeled training points at the beginning of the domain.\n",
    "\n",
    "This is sort of an unfair comparison though. We give both the MLP and PINN the same labeled points along the exact solution, but then provide the PINN with additional collaction points. Anyway, the point is to show that given a few points, we can recover the desired solution better using a PINN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants \n",
    "m = 1       # mass, kg\n",
    "w0 = 20     # natural frequency, rad/s (sqrt(k/m))\n",
    "k = w0**2   # spring constant, N/m\n",
    "d = 2       # damping ratio (mu/(2m))\n",
    "mu = 2*d*m  # damping coefficient, Ns/m\n",
    "\n",
    "# store the coefficients for the highest order derivatives\n",
    "coeffs = [k, mu, m]  # coefficientss on [u, ut, utt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IGNORE THIS CELL -- placeholder :) ##\n",
    "\n",
    "# set the collocation points\n",
    "coll = Collocation(t_physics, value=0.0, coefficients=coeffs, highest_order=2, reg_coeff=1e-4)\n",
    "\n",
    "# set the boundary conditions\n",
    "bc1 = BoundaryCondition(t_boundary, value=1.0, reg_coeff=1.0)\n",
    "bc2 = BoundaryCondition(t_boundary, deriv_value=0.0, reg_coeff=1e-4)\n",
    "bcs = [bc1, bc2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the PINN and MLP\n",
    "pinn = PINN(\n",
    "    num_input=1,\n",
    "    num_hidden=32,\n",
    "    num_output=1, \n",
    "    num_layers=3,\n",
    "    collocation=coll,\n",
    "    boundary_conditions=bcs\n",
    "    ).to(device)\n",
    "    \n",
    "mlp = MLP(1, 32, 1, num_layers=3).to(device)\n",
    "\n",
    "# set the optimizers\n",
    "pinn_opt = torch.optim.Adam(pinn.parameters(),lr=1e-3)\n",
    "mlp_opt = torch.optim.Adam(mlp.parameters(),lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, create some noisy observational data\n",
    "torch.manual_seed(123)\n",
    "t = torch.linspace(0,1,500).view(-1,1)\n",
    "u_exact = exact_solution(d, w0, t)\n",
    "\n",
    "# get a sample of the exact solution\n",
    "t_data = t[0:200:20]\n",
    "u_data = u_exact[0:200:20] \n",
    "\n",
    "# get collocation points\n",
    "t_physics = torch.linspace(0,1,30).view(-1,1).requires_grad_(True)\n",
    "\n",
    "# print(t.shape)\n",
    "# print(u_exact.shape)\n",
    "# print(t_data.shape)\n",
    "# print(u_data.shape)\n",
    "\n",
    "plt.figure(figsize=(6,2.5))\n",
    "plt.plot(t[:,0], u_exact[:,0], color=\"grey\", linewidth=2, alpha=0.8, label=\"Exact solution\")\n",
    "plt.scatter(t_data[:,0], u_data[:,0], s=60, color=\"tab:orange\", alpha=0.4, label='Training data')\n",
    "plt.scatter(t_physics.detach().cpu().numpy(), -0*torch.ones_like(t_physics.detach().cpu()), s=30, color=\"tab:green\", alpha=0.4, label=\"Collocation points\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(pinn_out, mlp_out, t_physics, t_data, t, u_exact, i):\n",
    "    plt.figure(figsize=(12, 2.5))  # Adjusted figure size to accommodate two subplots\n",
    "\n",
    "    # subplot for MLP prediction\n",
    "    plt.subplot(1, 2, 1)  \n",
    "    plt.plot(t[:, 0], u_exact[:, 0], color=\"grey\", linewidth=2, alpha=0.8, label=\"Exact solution\")\n",
    "    plt.plot(t[:, 0], mlp_out, color=\"tab:blue\", linewidth=4, alpha=0.8, label=\"MLP prediction\")\n",
    "    plt.scatter(t_data[:, 0], u_data[:, 0], s=60, color=\"tab:orange\", alpha=0.4, label=\"Training data\")\n",
    "    plt.title(f\"MLP Prediction at Step {i}\")\n",
    "    # set ylim -- the x lim is basically fixed\n",
    "    plt.ylim(-0.8, 1.1)\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"u(t)\")\n",
    "    plt.legend()\n",
    "\n",
    "    # subplot for PINN prediction\n",
    "    plt.subplot(1, 2, 2)  \n",
    "    plt.plot(t[:, 0], u_exact[:, 0], color=\"grey\", linewidth=2, alpha=0.8, label=\"Exact solution\")\n",
    "    plt.plot(t[:, 0], pinn_out, color=\"tab:blue\", linewidth=4, alpha=0.8, label=\"PINN prediction\")\n",
    "    plt.scatter(t_data[:, 0], u_data[:, 0], s=60, color=\"tab:orange\", alpha=0.4, label=\"Training data\")\n",
    "    plt.scatter(t_physics, -0 * torch.ones_like(t_physics), s=30, color=\"tab:green\", alpha=0.4, label=\"Collocation points\")\n",
    "    plt.title(f\"PINN Prediction at Step {i}\")\n",
    "    plt.ylim(-0.8, 1.1)\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"u(t)\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "files = []\n",
    "\n",
    "num_iters = 7001\n",
    "for i in range(num_iters):\n",
    "\n",
    "    ### MLP optimization step ###\n",
    "    mlp_opt.zero_grad()\n",
    "    \n",
    "    # MLP trains on the labeled data alone\n",
    "    mlp_out = mlp(t_data)\n",
    "    mlp_loss = torch.mean((mlp_out - u_data)**2)\n",
    "\n",
    "    mlp_loss.backward()\n",
    "    mlp_opt.step()\n",
    "\n",
    "\n",
    "    ### PINN optimization step ####\n",
    "    pinn_opt.zero_grad()\n",
    "    \n",
    "    # compute the labeled training data loss\n",
    "    pinn_out = pinn(t_data)\n",
    "    data_loss = torch.mean((pinn_out - u_data)**2)\n",
    "\n",
    "    # compute physics loss\n",
    "    u = pinn(t_physics)# (30, 1)\n",
    "    ut = torch.autograd.grad(u, t_physics, torch.ones_like(u).to(device), create_graph=True)[0]# (30, 1)\n",
    "    utt = torch.autograd.grad(ut, t_physics, torch.ones_like(ut), create_graph=True)[0]# (30, 1)\n",
    "    phys_loss = torch.mean((utt + mu*ut + k*u)**2) # residual of the PDE\n",
    "    \n",
    "    # backpropagate joint loss, take optimizer step\n",
    "    lam = 1e-4\n",
    "    pinn_loss = data_loss + lam*phys_loss\n",
    "    pinn_loss.backward()\n",
    "    pinn_opt.step()\n",
    "    \n",
    "    print(f\"MLP loss: {mlp_loss.item():.4f} | PINN loss: {pinn_loss.item():.4f} | step: {i} / {num_iters}\", end=\"\\r\")\n",
    "    # plot the result as training progresses\n",
    "    if i % 30 == 0: \n",
    "        # get the output on the full t domain\n",
    "        pinn_out = pinn(t)\n",
    "        mlp_out = mlp(t)\n",
    "\n",
    "        plot_result(\n",
    "            pinn_out = pinn_out.detach().cpu(), \n",
    "            mlp_out = mlp_out.detach().cpu(),\n",
    "            t_physics = t_physics.detach().cpu(), \n",
    "            t_data = t_data.detach().cpu(),\n",
    "            t = t.detach().cpu(),\n",
    "            u_exact = u_exact.detach().cpu(),\n",
    "            i=i\n",
    "        )\n",
    "\n",
    "        file = f\"pinn_{i+1}.png\"\n",
    "        save_fig(plt, file)\n",
    "        files.append(file)\n",
    "        \n",
    "        if (i+1) % 6000 == 0: plt.show()\n",
    "        else: plt.close(\"all\")\n",
    "            \n",
    "save_gif_PIL(\"pinn3.gif\", files, fps=20, loop=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We actually see that the MLP overfits on the limited data, getting a smaller loss than the PINN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"pinn3.gif\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML('<img src=\"pinn3.gif\" />'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "piml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
