{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Theory\n",
    "\n",
    "---\n",
    "\n",
    "Let's learn how PINNs work! In this notebook, we'll implement a PINN following one of the [original papers](https://arxiv.org/pdf/1711.10561). This [video](https://www.youtube.com/watch?v=IDIv92Z6Qvc) was also helpful\n",
    "\n",
    "The goal is to find a pde solution\n",
    "looks cool: https://arxiv.org/pdf/2403.00599\n",
    "\n",
    "https://github.com/idrl-lab/PINNpapers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 1D viscous  Burgers' equation is\n",
    "\n",
    "$$\n",
    "\\frac{\\partial u}{\\partial t} + u\\frac{\\partial u}{\\partial x} = \\nu \\frac{\\partial^2 u}{\\partial x^2} \\qquad x \\in [-1, 1], \\quad t \\in [0, 1]\n",
    "$$\n",
    "\n",
    "where $u(x, t)$ gives the wave displacement and $\\nu$ is the diffusion coefficient, describing\n",
    "\n",
    "We will solve it with the Dirichlet boundary conditions \n",
    "\n",
    "$$\n",
    "u(-1,t)=u(1,t)=0\n",
    "$$\n",
    "\n",
    "and initial conditions\n",
    "\n",
    "$$\n",
    "u(x,0) = - \\sin(\\pi x).\n",
    "$$\n",
    "\n",
    "First, we need to rewrite the PDE and its boundary / initial conditions into the form\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{D}[u(x, t)] &= f(x, t), \\quad x \\in \\Omega \\subset \\mathbb{R}^d \\\\\n",
    "B_k[u(x, t)] &= g_k(x, t), \\quad x \\in \\Gamma_k \\subset \\partial \\Omega \\\\\n",
    "I_l[u(x, t=0)] &= h_l(x, t=0), \\quad x \\in \\Omega \\subset \\mathbb{R}^d \\\\\n",
    "\\end{align*}\n",
    "\n",
    "as described in [this notebook](./spring.ipynb). The PDE becomes\n",
    "\n",
    "\n",
    "$$\n",
    "\\left( \\frac{\\partial }{\\partial t} + u\\frac{\\partial }{\\partial x} - \\nu \\frac{\\partial^2 }{\\partial x^2} \\right) \\big[u \\big] - 0 = 0 \n",
    "$$\n",
    "\n",
    "The boundary conditions remain the same since they equal zero, but we will explicitly write them out. So the boundary / initial conditions are\n",
    "\n",
    "\\begin{align*}\n",
    "1)& \\quad u(-1,t) = 0 \\quad \\Rightarrow \\quad u(-1,t) - 0 = 0 \\\\\n",
    "2)& \\quad u(1,t) = 0 \\quad \\Rightarrow \\quad u(1,t) - 0 = 0 \\\\\n",
    "3)& \\quad u(x,t_\\text{min}) = - \\sin(\\pi x) \\quad \\Rightarrow \\quad u(x,t_\\text{min}) + \\sin(\\pi x) = 0 \\\\\n",
    "\n",
    "\\end{align*}\n",
    "\n",
    "The PINN is trained to directly approximate the solution to the differential equation\n",
    "\n",
    "$$\n",
    "N\\!N(x, t;\\theta) \\approx u(x, t)\n",
    "$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Building the loss function\n",
    "\n",
    "The supervised **boundary loss** $ L_B(\\theta)$ is given by\n",
    "$$\n",
    "L_B(\\theta) = \\sum_{k} \\frac{\\lambda_k}{N_{Bk}} \\sum_{j}^{N_{Bk}} \\left\\| B_k \\left[ NN(x_{kj}, t_j; \\theta) \\right] - g_k(x_{kj}, t_j) \\right\\|^2 \n",
    "$$\n",
    "\n",
    "where the outer summation with index $k$ is over the various given boundary conditions $B_k$ on the PDE. $N_{Bk}$ is the number of *boundary* training points $x_{kj}$ for boundary condition $B_k$. Then the inner summation over $j$ sums the $N_{Bk}$ squared residuals, and $\\lambda_k$ is the regularization coefficient for the corresponding boundary condition.\n",
    "\n",
    "The supervised **initial condition loss** $ L_I(\\theta)$ is given by\n",
    "$$\n",
    "L_I(\\theta) = \\sum_{l} \\frac{\\lambda_l}{N_{Il}} \\sum_{j}^{N_{Il}} \\left\\| I_l \\left[ NN(x_{lj}, t_j; \\theta) \\right] - h_l(x_{lj}, t_j) \\right\\|^2 \n",
    "$$\n",
    "\n",
    "with the same idea as above except that now $t_j$ is really $t_\\text{min}$, the initial time.\n",
    "\n",
    "The unsupervised **physics loss** $ L_P(\\theta)$ is given by \n",
    "$$\n",
    "L_P(\\theta) = \\frac{1}{N_P} \\sum_{i}^{N_P} \\left\\| \\mathcal{D} \\left[ NN(x_i, t_i; \\theta) \\right] - f(x_i, t_i) \\right\\|^2 \n",
    "$$\n",
    "\n",
    "where we sum the $N_P$ squared residuals.\n",
    "\n",
    "Then our regularized loss function is made up of the physics loss, $L_P(\\theta)$, the boundary loss, $L_B(\\theta)$, and the initial condition loss, $L_I(\\theta)$, with $t_\\text{min} = 0$\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(\\theta) &= \\lambda_P L_P(\\theta) + \\sum_k \\lambda_{Bk} L_{Bk}(\\theta) + \\lambda_I L_I(\\theta) \\\\\n",
    "&= \\frac{\\lambda_P}{N_p} \\sum_{i=1}^{N_P} \\left( \\left[\\frac{\\partial }{\\partial t} + N\\!N(x_i, t_i; \\theta) \\frac{\\partial }{\\partial x} - \\nu \\frac{\\partial^2 }{\\partial x^2} \\right] N\\!N (x_i, t_i; \\theta)\\right)^2 \\\\\n",
    "&+ \\frac{\\lambda_{B1}}{N_{B1}} \\sum_{j=1}^{N_{B1}} \\left( N\\!N(-1, t_j; \\theta) - 0 \\right)^2 + \\frac{\\lambda_{B2}}{N_{B2}} \\sum_{j=1}^{N_{B2}} \\left(N\\!N(1, t_j; \\theta) - 0\\right)^2 \\\\\n",
    "&+ \\frac{\\lambda_I}{N_I} \\sum_{k=1}^{N_I} \\left( N\\!N(x_k, 0; \\theta) + \\sin(\\pi x_k) \\right)^2\n",
    "\\end{align*}\n",
    "\n",
    "which we can simplify to\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(\\theta) &= \\lambda_P L_P(\\theta) + \\sum \\lambda_B L_B(\\theta) + \\lambda_I L_I(\\theta) \\\\\n",
    "&= \\frac{\\lambda_P}{N_p} \\left\\| \\left[\\frac{\\partial }{\\partial t} + N\\!N(\\vec{x}_P, \\vec{t}_P; \\theta) \\frac{\\partial }{\\partial x} - \\nu \\frac{\\partial^2 }{\\partial x^2} \\right] N\\!N (\\vec{x}_P, \\vec{t}_P; \\theta) \\right\\|^2\\\\\n",
    "&+ \\frac{\\lambda_{B1}}{N_{B1}} \\left\\| N\\!N(-1, \\vec{t}_{B1}; \\theta) \\right\\|^2 + \\frac{\\lambda_{B2}}{N_{B2}} \\left\\|N\\!N(1, \\vec{t}_{B2}; \\theta) \\right\\|^2 \\\\\n",
    "&+ \\frac{\\lambda_I}{N_I} \\left\\| N\\!N(\\vec{x}_I, 0; \\theta) + \\sin(\\pi \\vec{x}_I) \\right\\|^2\n",
    "\\end{align*}\n",
    "\n",
    "where \n",
    "- $\\vec{x}_P$ and $\\vec{t}_P$ belong to the set of collocation points $\\{(x, t)_i\\}_{i=1}^{N_P}$\n",
    "- $\\vec{t}_{Bk}$ belongs to the set of boundary condition points $\\{(g_k, t)_i\\}_{i=1}^{N_{Bk}}$\n",
    "- $\\vec{x}_I$ belongs to the set of initial condition points $\\{(x, t_\\text{min})_i\\}_{i=1}^{N_I}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Adding a transform layer\n",
    "\n",
    "It looks like we have a few regularization hyperparameters to choose... one way we can remove some of these is by adding a transform function after the output layer of the network. This transform layer would enforce the initial condition at $t_\\text{min}$ and look something like\n",
    "\n",
    "```py\n",
    "def transform_output(x, t, u, Tmin, h):\n",
    "    return h(x, t) + u(x, t) * (t - Tmin) * (1 - x**2)\n",
    "```\n",
    "\n",
    "where `x` and `t` are the inputs to the PINN, `u` is the PINN output approximation of the PDE, `Tmin` is the initial condition time, and `h` is the initial condition function itself. As an equation,\n",
    "\n",
    "$$\n",
    "u_{\\text{transformed}}(x, t) = h(x, t) + NN(x, t; \\theta) \\cdot (t - t_{\\text{min}}) \\cdot (1 - x^2)\n",
    "$$\n",
    "\n",
    "where, again, we have the PDE solution approximation $N\\!N(x, t;\\theta) \\approx u(x, t)$. The idea behind this is simple: the transformation ensures that our initial condition function $h(x, t)$ is satisfied when $t = t_\\text{min}$. In all other cases, when $t > t_\\text{min}$, we see that the output is a combination of the specified initial condition function and the untransformed network output. In addition to enforcing the initial condition, it also satisfies the boundary conditions by making the output $0$ when $x = \\pm 1$ for our particular initial condition $h(x, t) = -\\sin(\\pi x)$.\n",
    "\n",
    "So by using this transform layer, we get to remove the initial condition loss term from our overall loss function -- simplifying it and giving us one less hyperparameter to tune :)\n",
    "\n",
    "However, the transform layer has no learnable parameters and is technically not part of the neural network. But since it sits at the end of the network, it will have a strong influence on its output and, thus, the loss. This means that errors backpropagating through the network will tune its weights with information about the initial conditions recieved not explicitly from the loss function, but from the transform layer itself.\n",
    "\n",
    "If we consider the transform layer as part of the network, we can write our new PDE solution approximation as\n",
    "$$\n",
    "N\\!N'(x, t;\\theta) \\approx u_{\\text{transformed}}(x, t, NN(x, t; \\theta))\n",
    "$$\n",
    "\n",
    "where $N\\!N'$ is the output of the network *with* the transform layer, and $N\\!N$ is the previous vanilla output (included explicitly in the transformed solution here).\n",
    "\n",
    "Then our new loss function becomes\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(\\theta) &= \\lambda_P L_P(\\theta) + \\sum \\lambda_B L_B(\\theta)  \\\\\n",
    "&= \\frac{\\lambda_P}{N_p} \\left\\| \\left[\\frac{\\partial }{\\partial t} + N\\!N'(x_i, t_i; \\theta) \\frac{\\partial }{\\partial x} - \\nu \\frac{\\partial^2 }{\\partial x^2} \\right] N\\!N' (x_i, t_i; \\theta) \\right\\|^2\\\\\n",
    "&+ \\frac{\\lambda_{B1}}{N_{B1}} \\left\\| N\\!N'(-1, t_j; \\theta) \\right\\|^2 + \\frac{\\lambda_{B2}}{N_{B2}} \\left\\|N\\!N'(1, t_j; \\theta) \\right\\|^2 \n",
    "\\end{align*}\n",
    "\n",
    "with final layer\n",
    "$$\n",
    "NN'(x, t; \\theta) = -\\sin(\\pi x) + NN(x, t; \\theta) \\cdot (t - t_{\\text{min}}) \\cdot (1 - x^2)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Finally getting to some code!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "from typing import Callable, List, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "# elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "#     device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're mainly going to be storing sets of $(x, t)$ tuples as $N \\times 2$ tensors `xt`, where each row represents a sample, `xt[:, 0]` are the $x$ values, and `xt[:, 1]` are the $t$ values.\n",
    "\n",
    "When indexing a column `i` in a tensor `a`, we can use `a[:, i:i+1]` instead of `a[:, i]` to preserve the column shape, if we so desire. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1],\n",
      "        [2, 3],\n",
      "        [4, 5],\n",
      "        [6, 7],\n",
      "        [8, 9]])\n",
      "tensor([1, 3, 5, 7, 9])\n",
      "tensor([[1],\n",
      "        [3],\n",
      "        [5],\n",
      "        [7],\n",
      "        [9]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(10).reshape(5, 2)\n",
    "print(a)\n",
    "print(a[:, 1])\n",
    "print(a[:, 1:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Defining the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need a way of representing the boundary and intial conditions for our PDE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: maybe store interval endpoints as well\n",
    "# TODO: derivatives... see spring.ipynb for some ideas on this\n",
    "class BC():\n",
    "    \"\"\"Boundary condition\"\"\"\n",
    "    def __init__(self, x: float, f: Callable):\n",
    "        self.x = x    # boundary location\n",
    "        self.f = f    # boundary condition function\n",
    "\n",
    "class IC():\n",
    "    \"\"\"Initial condition\"\"\"\n",
    "    def __init__(self, Tmin: float, f: Callable):\n",
    "        self.Tmin = Tmin  # initial time, usually just 0\n",
    "        self.f = f        # initial condition function\n",
    "\n",
    "# lhs and rhs have form lhs = lambda x, t: ...\n",
    "class PDE():\n",
    "    \"\"\"PDE equation for the physics loss\"\"\"\n",
    "    def __init__(self, LHS: Callable, RHS: Callable):\n",
    "        self.LHS = LHS\n",
    "        self.RHS = RHS\n",
    "\n",
    "    def __call__(self, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        return self.LHS(x, t) - self.RHS(x, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next we create our `transform` function to append to the output of the network, along with the initial condition `h`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def old_transform(\n",
    "        x: torch.Tensor, \n",
    "        t: torch.Tensor, \n",
    "        u: torch.Tensor, \n",
    "        Tmin: float, \n",
    "        h: Callable\n",
    "        ) -> torch.Tensor:\n",
    "    \"\"\"Transform layer enforcing the initial condition\"\"\"\n",
    "\n",
    "    def h(x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Initial condition\"\"\"\n",
    "        return -torch.sin(torch.pi * x) \n",
    "    \n",
    "    ones = torch.ones_like(x)\n",
    "    ts = t * torch.ones_like(t)\n",
    "    return h(x, t) + u * (ts - Tmin) * (ones - x**2)\n",
    "\n",
    "\n",
    "def transform(xt: torch.Tensor, u: torch.Tensor, ic: IC) -> torch.Tensor:\n",
    "    \"\"\"Transform layer enforcing the initial condition\"\"\"\n",
    "    # get the x and t inputs to the network\n",
    "    x = xt[:, 0:1]\n",
    "    t = xt[:, 1:2]\n",
    "    ones = torch.ones_like(x)\n",
    "    ts = t * torch.ones_like(t)\n",
    "    Tmins = ic.Tmin * torch.ones_like(t)\n",
    "    return ic.f(x, t) + u * (ts - Tmins) * (ones - x**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to define our `PINN`! We will initialize our weights using [Glorot / Xavier initialization](https://mmuratarat.github.io/2019-02-25/xavier-glorot-he-weight-init), and allow for the use of a custom, optional `transform_layer`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(nn.Module):\n",
    "    \"\"\"PINN model for specifically solving Burgers' equation\"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            layers: List[int],\n",
    "            bcs: List[BC],\n",
    "            ics: List[IC],\n",
    "            activation: Callable = nn.Tanh,\n",
    "            transform_layer: Callable = None,\n",
    "            loss_fn: Callable = nn.MSELoss\n",
    "            ):\n",
    "        super(PINN, self).__init__()\n",
    "        self.bcs = bcs                     \n",
    "        self.ics = ics                     \n",
    "        self.activation = activation()     # TODO: i don't like doing act() here, see if i can do act\n",
    "        self.transform_layer = transform_layer\n",
    "        self.loss_fn = loss_fn\n",
    "        \n",
    "        # define the MLP layers and initialize the weights\n",
    "        self.fc = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self._xavier_init()\n",
    "\n",
    "    def _xavier_init(self):\n",
    "        for layer in self.fc:\n",
    "            nn.init.xavier_normal_(layer.weight)\n",
    "            nn.init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self, xt: torch.Tensor) -> torch.Tensor:\n",
    "        # only save input if transform is provided\n",
    "        if self.transform_layer is not None:\n",
    "            input = xt.clone().detach().requires_grad_(True)  # TODO: check this later\n",
    "        \n",
    "        for layer in self.fc:\n",
    "            xt = self.activation(layer(xt))\n",
    "\n",
    "        # transform output if provided\n",
    "        if self.transform_layer is not None:\n",
    "            # the input to the network was xt, and the output is now xt... # TODO: better notation\n",
    "            return self.transform_layer(xt=input, u=xt, ic=self.ics[0])\n",
    "        return xt\n",
    "    \n",
    "    def physics_loss(self, xt: torch.Tensor, lam: float, nu: float) -> torch.Tensor:\n",
    "        \"\"\"Get the loss dictated by the PDE\"\"\"\n",
    "        # get the networks predictions\n",
    "        u = self.forward(xt)\n",
    "\n",
    "        # get the gradients: x = xt[:, 0:1], t = xt[:, 1:2]\n",
    "        ut = torch.autograd.grad(u, xt[:, 1:2], torch.ones_like(u), create_graph=True)[0]\n",
    "        ux = torch.autograd.grad(u, xt[:, 0:1], torch.ones_like(u), create_graph=True)[0]\n",
    "        uxx = torch.autograd.grad(ux, xt[:, 1:2], torch.ones_like(ux), create_graph=True)[0]\n",
    "        \n",
    "        # put the terms into the PDE and get the residual\n",
    "        residual = lam * (ut + u*ux - nu*uxx)\n",
    "        return torch.mean(residual**2)\n",
    "        # return self.loss_fn(lam * (ut + u*ux - nu*uxx), torch.zeros_like(u))\n",
    "    \n",
    "\n",
    "    # TODO: the ic and bc losses are really the same template -- combine these into one function later\n",
    "    # TODO: also annoying to generate x's or t's for the losses each time\n",
    "    #       - could maybe index sample points with (x_bc, t) and (x, t_ic) and pass those in instead\n",
    "    def bc_loss(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Get the loss for the boundary conditions -- xs and us are lists of x and u values for the BCs\"\"\"\n",
    "        loss = 0\n",
    "        for bc in self.bcs:\n",
    "            x = bc.x * torch.ones_like(t, requires_grad=True) # make x the same size as t\n",
    "            xt = torch.cat((x, t), dim=1)                     # concat along dim=1 to make the x and t columns of xt\n",
    "            u = self.forward(xt)                              # get the networks prediction\n",
    "            targets = bc.f(x, t)                              # get true BC value\n",
    "            loss += self.loss_fn(u, targets)                  # add the loss to the total\n",
    "        return loss\n",
    "    \n",
    "    def ic_loss(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Get the loss for the initial conditions\"\"\"\n",
    "        loss = 0\n",
    "        for ic in self.ics:\n",
    "            t = ic.Tmin * torch.ones_like(x, requires_grad=True) # make t the same size as x\n",
    "            xt = torch.cat((x, t), dim=1)                     # concat along dim=1 to make the x and t columns of xt\n",
    "            u = self.forward(xt)                              # get the networks prediction \n",
    "            targets = ic.f(x, t)                              # get true IC value\n",
    "            loss += self.loss_fn(u, targets)                  # add the loss to the total\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set our boundary and initial conditions. Recall, we had\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{cases}\n",
    "\\text{BC1}: & u(-1, t) = 0, \\\\\n",
    "\\text{BC2}: & u(1, t) = 0, \\\\\n",
    "\\text{IC1}: & u(x, t_{\\min}) = -\\sin(\\pi x)\n",
    "\\end{cases}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc1 = BC(x=-1, f=lambda x, t: 0)\n",
    "bc2 = BC(x=1, f=lambda x, t: 0)\n",
    "bcs = [bc1, bc2]\n",
    "\n",
    "ic = IC(Tmin=0, f=lambda x, t: -torch.sin(torch.pi * x))\n",
    "ics = [ic]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define our PDE. \n",
    "\n",
    "$$\n",
    "\\frac{\\partial u}{\\partial t} + u\\frac{\\partial u}{\\partial x} = \\nu \\frac{\\partial^2 u}{\\partial x^2} \\qquad x \\in [-1, 1], \\quad t \\in [0, 1]\n",
    "$$\n",
    "\n",
    "> Currently have no framework for representing derivatives outside of the network, so this part is not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LHS = lambda x, t: 0\n",
    "RHS = lambda x, t: 0\n",
    "pde = PDE(LHS, RHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can instantiate our network. The input layer should accecpt tensors of shape $(2, )$, as we'll be passing in tuples $(x, t)$. The output layer should only contain a single neuron, since the network output serves as the solution to the PDE, $u(x, t)$.\n",
    "\n",
    "The optimizer ... LBFGS.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 921\n"
     ]
    }
   ],
   "source": [
    "# pinn = PINN([2, 20, 20, 20, 1], transform)\n",
    "pinn = PINN([2, 20, 20, 20, 1], bcs=bcs, ics=ics).to(device)\n",
    "opt = torch.optim.Adam(pinn.parameters(), lr=0.001)\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in pinn.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print a summary of the model by passing in a mock tensor of size $(N, 2)$, where $N$ is the number of samples being processed in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1               [-1, 10, 20]              60\n",
      "              Tanh-2               [-1, 10, 20]               0\n",
      "            Linear-3               [-1, 10, 20]             420\n",
      "              Tanh-4               [-1, 10, 20]               0\n",
      "            Linear-5               [-1, 10, 20]             420\n",
      "              Tanh-6               [-1, 10, 20]               0\n",
      "            Linear-7                [-1, 10, 1]              21\n",
      "              Tanh-8                [-1, 10, 1]               0\n",
      "================================================================\n",
      "Total params: 921\n",
      "Trainable params: 921\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.01\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(pinn, input_size=(10, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create our supervised initial & boundary points, and unsupervised physics / collocation points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define boundary points, for the boundary loss\n",
    "bc1_t = torch.linspace(0, 1, 10).view(-1, 1).requires_grad_(True).to(device)\n",
    "bc2_t = torch.linspace(0, 1, 10).view(-1, 1).requires_grad_(True).to(device)\n",
    "\n",
    "# t_boundary = torch.Tensor(0.).view(-1,1).requires_grad_(True).to(device)# (1, 1)\n",
    "\n",
    "\n",
    "# define initial condition points, for the initial condition loss\n",
    "ic_x = torch.linspace(-1, 1, 10).view(-1, 1).requires_grad_(True).to(device)\n",
    "\n",
    "# define collocation points over the entire domain, for the physics loss\n",
    "col_x = torch.rand(10, 1).requires_grad_(True).to(device)\n",
    "# t_physics = torch.linspace(0,1,30).view(-1,1).requires_grad_(True).to(device)# (30, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: figure out beset way to setup grid\n",
    "# generate collocation points\n",
    "x_collocation = torch.linspace(-1, 1, 1000, device=device)\n",
    "t_collocation = torch.linspace(0, 1, 1000, device=device)\n",
    "x_collocation, t_collocation = torch.meshgrid(x_collocation, t_collocation, indexing=\"ij\")\n",
    "x_collocation = x_collocation.flatten().unsqueeze(1).to(device)\n",
    "t_collocation = t_collocation.flatten().unsqueeze(1).to(device)\n",
    "\n",
    "# generate boundary points\n",
    "t_boundary = torch.linspace(0, 1, 1000, device=device).unsqueeze(1).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can write the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimization_step(pinn, opt, x, t, tmin):\n",
    "    opt.zero_grad()\n",
    "    input = torch.cat([x, t], dim=1)\n",
    "    output = pinn(input)\n",
    "    loss = pinn.ic_loss(x) + pinn.bc_loss(t) + pinn.physics_loss(input, 1, 0.01)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's find a good learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: figure out how to use with model\n",
    "g = torch.Generator().manual_seed(2147483647)  # rng seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1000 exponents evenly spaced between -3 and 0\n",
    "lre = torch.linspace(-3, 0, steps=1000)\n",
    "lrs = 10**lre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lossi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# find the minimum loss and the corresponding learning rate\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m min_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[43mlossi\u001b[49m)\n\u001b[1;32m      3\u001b[0m min_loss_index \u001b[38;5;241m=\u001b[39m lossi\u001b[38;5;241m.\u001b[39mindex(min_loss)\n\u001b[1;32m      4\u001b[0m min_lrei \u001b[38;5;241m=\u001b[39m lrei[min_loss_index]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lossi' is not defined"
     ]
    }
   ],
   "source": [
    "# find the minimum loss and the corresponding learning rate\n",
    "min_loss = min(lossi)\n",
    "min_loss_index = lossi.index(min_loss)\n",
    "min_lrei = lrei[min_loss_index]\n",
    "\n",
    "min_loss = min(lossi)\n",
    "min_loss_index = lossi.index(min_loss)\n",
    "min_lr = lrs[min_loss_index]\n",
    "print(f\"Min learning rate exponent: {min_lrei:.2f} and corresponding learing rate: {10**min_lrei:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can plot the results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# loss vs learning rate exponent\n",
    "ax1.plot(lrei, lossi)\n",
    "ax1.axvline(x=min_lrei, color=\"r\", linestyle=\"--\", label=f\"Min loss at {min_lrei:.2f}\")\n",
    "ax1.set_xlabel(\"Learning Rate Exponent\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.set_title(\"Loss vs Learning Rate Exponent\")\n",
    "ax1.legend()\n",
    "\n",
    "# loss vs learning rate\n",
    "ax2.plot(lrs, lossi)\n",
    "ax2.axvline(x=min_lr, color=\"r\", linestyle=\"--\", label=f\"Min loss at {min_lr:.2e}\")\n",
    "ax2.set_xlabel(\"Learning Rate\")\n",
    "ax2.set_ylabel(\"Loss\")\n",
    "ax2.set_title(\"Loss vs Learning Rate\")\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "piml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
